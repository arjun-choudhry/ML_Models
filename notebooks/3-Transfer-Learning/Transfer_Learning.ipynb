{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e5c727-33d1-4b1c-b45c-d8d48beeb940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjunlfc/Documents/workspace/_venv/pytorch/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/arjunlfc/Documents/workspace/_mlmodels/\")\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "\n",
    "from notebooks.utils import save, data_setup, training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380e84c-54bf-4d35-81d6-2c34204fff2e",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1084189-bb5e-4617-b017-f4cfc5883f2c",
   "metadata": {},
   "source": [
    "There are several places you can find pretrained models to use for your own problems:\n",
    "- Pytorch domain Libraries: Each of the PyTorch domain libraries (torchvision, torchtext) come with pretrained models of some form. The models there work right within PyTorch. `torchvision.models, torchtext.models, torchaudio.models, torchrec.models`\n",
    "- HuggingFace Hub: A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. `https://huggingface.co/models, https://huggingface.co/datasets`\n",
    "- timm (PyTorch Image Models) library: Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. `https://github.com/rwightman/pytorch-image-models`\n",
    "- Paperswithcode: A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. `https://paperswithcode.com/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cb9391-6af4-4f5d-9769-906d00def0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    return 'cpu'\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db63fb2-c6e7-4ec2-b6f1-4ef5773d504f",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70752a88-a469-4a5f-8269-f0bf02c11221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../2-NLP-CV-Basics/datasets/pizza_steak_sushi directory exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../2-NLP-CV-Basics/datasets/pizza_steak_sushi/train',\n",
       " '../2-NLP-CV-Basics/datasets/pizza_steak_sushi/test')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_data(dir_path, filename):\n",
    "    # Setup path to data folder\n",
    "    data_path = Path(dir_path)\n",
    "    image_path = data_path / filename\n",
    "    \n",
    "    # If the image folder doesn't exist, download it and prepare it... \n",
    "    if image_path.is_dir():\n",
    "        print(f\"{image_path} directory exists.\")\n",
    "    else:\n",
    "        print(f\"Did not find {image_path} directory, creating one...\")\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Download pizza, steak, sushi data\n",
    "        with open(data_path / f\"{filename}.zip\", \"wb\") as f:\n",
    "            request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "            print(\"Downloading pizza, steak, sushi data...\")\n",
    "            f.write(request.content)\n",
    "    \n",
    "        # Unzip pizza, steak, sushi data\n",
    "        with zipfile.ZipFile(data_path / f\"{filename}.zip\", \"r\") as zip_ref:\n",
    "            print(\"Unzipping pizza, steak, sushi data...\") \n",
    "            zip_ref.extractall(image_path)\n",
    "\n",
    "DATASET_PATH=\"../2-NLP-CV-Basics/datasets/\"\n",
    "FILENAME=\"pizza_steak_sushi\"\n",
    "download_data(DATASET_PATH, FILENAME)\n",
    "\n",
    "# Setup train and testing paths\n",
    "train_dir = f\"{DATASET_PATH}{FILENAME}/train\"\n",
    "test_dir = f\"{DATASET_PATH}{FILENAME}/test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e2626-f4a5-430f-9826-af25f375abba",
   "metadata": {},
   "source": [
    "### Creating transform/dataloader for torchvision models\n",
    "\n",
    "#### 2.1 Creating Manual Transform\n",
    "When using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\n",
    "\n",
    "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "\n",
    "We can perform the above required transformation using the following:\n",
    "- torchvision.transforms.Resize() to resize images into [3, 224, 224]^ and torch.utils.data.DataLoader() to create batches of images.\n",
    "- torchvision.transforms.ToTensor() to change the values between 0 and 1\n",
    "- torchvision.transforms.Normalize(mean=..., std=...) to adjust the mean and standard deviation of our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c52c92-9584-45b8-895a-dca1a68dbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f362a05c-34bc-41e8-b902-0cef61286f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x12903f190>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x12903fa00>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
    "                                                                               batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293e01c-1227-46dc-aa5e-0671f84981c3",
   "metadata": {},
   "source": [
    "#### 2.2 Creating automatic transforms\n",
    "\n",
    "When using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model. As of torchvision v0.13+, an automatic transform creation feature has been added. When you setup a model from torchvision.models and select the pretrained model weights you'd like to use, for example, say we'd like to use:\n",
    "\n",
    "`weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT`\n",
    "\n",
    "Where,\n",
    "`EfficientNet_B0_Weights` is the model architecture weights we'd like to use (there are many different model architecture options in torchvision.models). `DEFAULT` means the best available weights (the best performance in ImageNet).\n",
    "Note: Depending on the model architecture you choose, you may also see other options such as IMAGENET_V1 and IMAGENET_V2 where generally the higher version number the better. Though if you want the best available, DEFAULT is the easiest option. \n",
    "\n",
    "And now to access the transforms associated with our weights, we can use the transforms() method.\n",
    "This is essentially saying \"get the data transforms that were used to train the EfficientNet_B0_Weights on ImageNet\".\n",
    "\n",
    "`auto_transforms = weights.transforms()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae17a0e1-54d0-4e27-9a36-090747920857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BICUBIC\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT \n",
    "auto_transforms = weights.transforms()\n",
    "print(auto_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7b986-91e0-4061-8f07-b12cf6eecc2c",
   "metadata": {},
   "source": [
    "The benefit of automatically creating a transform through weights.transforms() is that you ensure you're using the same data transformation as the pretrained model used when it was trained.\n",
    "\n",
    "However, the tradeoff of using automatically created transforms is a lack of customization.\n",
    "\n",
    "We can use auto_transforms to create DataLoaders with create_dataloaders() just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08921007-b9ab-4b3b-80a5-548040d847da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x12903f6a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x12903f700>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=auto_transforms, \n",
    "                                                                               batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b2f65-4761-4bfb-98b2-0bf1fe18138a",
   "metadata": {},
   "source": [
    "## Getting pretrained model\n",
    "\n",
    "The whole idea of transfer learning is to take an already well-performing model on a problem-space similar to yours and then customise it to your use case. Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in torchvision.models.\n",
    "\n",
    "Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:\n",
    "- ResNets: torchvision.models.resnet50()\n",
    "- VGG: torchvision.models.vgg16()\n",
    "- EfficientNets: torchvision.models.efficientnet_b0(), torchvision.models.efficientnet_b1()\n",
    "- ViT: torchvision.models.vit_b_16(), torchvision.models.vit_b_32()...\n",
    "- ConvNext: torchvision.models.convnext_tiny(), torchvision.models.convnext_small()...\n",
    "\n",
    "Generally, the higher number in the model name (e.g. efficientnet_b0() -> efficientnet_b1() -> efficientnet_b7()) means better performance but a larger model. Some better performing models are too big for some devices. Understanding this performance vs. speed vs. size tradeoff will come with time and practice. \n",
    "\n",
    "There is a nice balance in the efficientnet_bX models.\n",
    "As of May 2022, Nutrify (the machine learning powered app) is powered by an efficientnet_b0.\n",
    "Comma.ai (a company that makes open source self-driving car software) uses an efficientnet_b2 to learn a representation of the road.\n",
    "\n",
    "We're going to create, a pretrained EfficientNet_B0 model from torchvision.models with the output layer adjusted for our use case of classifying pizza, steak and sushi images. We can setup the EfficientNet_B0 pretrained ImageNet weights using the same code as we used to create the transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef43010-4aa6-4e27-a335-f9d434346e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569124f4-26cb-463c-8b5b-d9a1204da56e",
   "metadata": {},
   "source": [
    "`efficientnet_b0` comes in three main parts:\n",
    "- features - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as features or feature extractor, \"the base layers of the model learn the different features of images\").\n",
    "- avgpool - Takes the average of the output of the features layer(s) and turns it into a feature vector.\n",
    "- classifier - Turns the feature vector into a vector with the same dimensionality as the number of required output classes (since efficientnet_b0 is pretrained on ImageNet and because ImageNet has 1000 classes, out_features=1000 is the default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bae028-ae2d-41de-bbc6-90dc7608f111",
   "metadata": {},
   "source": [
    "#### Getting a summary of the model\n",
    "\n",
    "To learn more about our model, let's use torchinfo's summary() method.\n",
    "\n",
    "To do so, we'll pass in:\n",
    "- model - the model we'd like to get a summary of.\n",
    "- input_size - the shape of the data we'd like to pass to our model, for the case of efficientnet_b0, the input size is (batch_size, 3, 224, 224), though other variants of efficientnet_bX have different input sizes.\n",
    "    - Note: Many modern models can handle input images of varying sizes thanks to torch.nn.AdaptiveAvgPool2d(), this layer adaptively adjusts the output_size of a given input as required. You can try this out by passing different size input images to summary() or your models.\n",
    "- col_names - the various information columns we'd like to see about our model.\n",
    "- col_width - how wide the columns should be for the summary.\n",
    "- row_settings - what features to show in a row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ab7e04-dcbd-4410-8312-0c3f051c33af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 240, 240]    [32, 1000]           --                   True\n",
       "├─Sequential (features)                                      [32, 3, 240, 240]    [32, 1280, 8, 8]     --                   True\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 240, 240]    [32, 32, 120, 120]   --                   True\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 240, 240]    [32, 32, 120, 120]   864                  True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 120, 120]   [32, 32, 120, 120]   64                   True\n",
       "│    │    └─SiLU (2)                                         [32, 32, 120, 120]   [32, 32, 120, 120]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 120, 120]   [32, 16, 120, 120]   --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 32, 120, 120]   [32, 16, 120, 120]   1,448                True\n",
       "│    └─Sequential (2)                                        [32, 16, 120, 120]   [32, 24, 60, 60]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 16, 120, 120]   [32, 24, 60, 60]     6,004                True\n",
       "│    │    └─MBConv (1)                                       [32, 24, 60, 60]     [32, 24, 60, 60]     10,710               True\n",
       "│    └─Sequential (3)                                        [32, 24, 60, 60]     [32, 40, 30, 30]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 24, 60, 60]     [32, 40, 30, 30]     15,350               True\n",
       "│    │    └─MBConv (1)                                       [32, 40, 30, 30]     [32, 40, 30, 30]     31,290               True\n",
       "│    └─Sequential (4)                                        [32, 40, 30, 30]     [32, 80, 15, 15]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 40, 30, 30]     [32, 80, 15, 15]     37,130               True\n",
       "│    │    └─MBConv (1)                                       [32, 80, 15, 15]     [32, 80, 15, 15]     102,900              True\n",
       "│    │    └─MBConv (2)                                       [32, 80, 15, 15]     [32, 80, 15, 15]     102,900              True\n",
       "│    └─Sequential (5)                                        [32, 80, 15, 15]     [32, 112, 15, 15]    --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 80, 15, 15]     [32, 112, 15, 15]    126,004              True\n",
       "│    │    └─MBConv (1)                                       [32, 112, 15, 15]    [32, 112, 15, 15]    208,572              True\n",
       "│    │    └─MBConv (2)                                       [32, 112, 15, 15]    [32, 112, 15, 15]    208,572              True\n",
       "│    └─Sequential (6)                                        [32, 112, 15, 15]    [32, 192, 8, 8]      --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 112, 15, 15]    [32, 192, 8, 8]      262,492              True\n",
       "│    │    └─MBConv (1)                                       [32, 192, 8, 8]      [32, 192, 8, 8]      587,952              True\n",
       "│    │    └─MBConv (2)                                       [32, 192, 8, 8]      [32, 192, 8, 8]      587,952              True\n",
       "│    │    └─MBConv (3)                                       [32, 192, 8, 8]      [32, 192, 8, 8]      587,952              True\n",
       "│    └─Sequential (7)                                        [32, 192, 8, 8]      [32, 320, 8, 8]      --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 192, 8, 8]      [32, 320, 8, 8]      717,232              True\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 8, 8]      [32, 1280, 8, 8]     --                   True\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 8, 8]      [32, 1280, 8, 8]     409,600              True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 8, 8]     [32, 1280, 8, 8]     2,560                True\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 8, 8]     [32, 1280, 8, 8]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 8, 8]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n",
       "============================================================================================================================================\n",
       "Total params: 5,288,548\n",
       "Trainable params: 5,288,548\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 14.80\n",
       "============================================================================================================================================\n",
       "Input size (MB): 22.12\n",
       "Forward/backward pass size (MB): 4011.39\n",
       "Params size (MB): 21.15\n",
       "Estimated Total Size (MB): 4054.67\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model, \n",
    "        input_size=(32, 3, 240, 240), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d421069-ed58-4683-9919-a2d94b065100",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "### Freezing the base layers and customizing the classification layers\n",
    "\n",
    "The process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the features section) and then adjust the output layers (also called head/classifier layers) to suit your needs. The original torchvision.models.efficientnet_b0() comes with out_features=1000 because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need out_features=3. \n",
    "\n",
    "We can freeze all of the layers/parameters in the features section by setting the attribute requires_grad=False.\n",
    "\n",
    "For parameters with requires_grad=False, PyTorch doesn't track gradient updates and in turn, these parameters won't be changed by our optimizer during training.\n",
    "\n",
    "In essence, a parameter with requires_grad=False is \"untrainable\" or \"frozen\" in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "004074c0-71b5-45d4-b4ae-8b80815ebca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd422b5-26c9-4e9a-937b-9fff2059451c",
   "metadata": {},
   "source": [
    "Let's now adjust the output layer or the classifier portion of our pretrained model to our needs.\n",
    "\n",
    "Right now our pretrained model has out_features=1000 because there are 1000 classes in ImageNet. \n",
    "The current classifier consists of:\n",
    "```\n",
    "(classifier): Sequential(\n",
    "    (0): Dropout(p=0.2, inplace=True)\n",
    "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
    "```\n",
    "\n",
    "We'll keep the Dropout layer the same using `torch.nn.Dropout(p=0.2, inplace=True)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b87df0b-fbb7-4865-9685-beea40ceac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of class_names (one output unit for each class)\n",
    "output_shape = len(class_names)\n",
    "\n",
    "# Recreate the classifier layer and seed it to the target device\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=1280, \n",
    "                    out_features=output_shape, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56f392a3-aa9a-4a1f-a096-8a18317f9b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
       "============================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (G): 12.31\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
    "summary(model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b569f-d1fa-4150-b6ce-7245cd224847",
   "metadata": {},
   "source": [
    "Let's go through them:\n",
    "\n",
    "- Trainable column - You'll see that many of the base layers (the ones in the features portion) have their Trainable value as False. This is because we set their attribute requires_grad=False. Unless we change this, these layers won't be updated during future training.\n",
    "- Output shape of classifier - The classifier portion of the model now has an Output Shape value of [32, 3] instead of [32, 1000]. It's Trainable value is also True. This means its parameters will be updated during training. In essence, we're using the features portion to feed our classifier portion a base representation of an image and then our classifier layer is going to learn how to base representation aligns with our problem.\n",
    "- Less trainable parameters - Previously there were 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the classifier as trainable, there's now only 3,843 trainable parameters (even less than our TinyVGG model). Though there's also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our classifier layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91b3329d-6fd0-4f04-9862-5573c9be36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7acdce72-7115-42c2-a998-1405cabcb566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da6c87-a599-4d6b-99c9-81c01aa9bea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1bdd3648794473b23ad2004acc317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup training and save the results\n",
    "results = training.train(model=model,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=5,\n",
    "                       device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788e9e0-c7ec-4dee-a395-673eea91910d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
